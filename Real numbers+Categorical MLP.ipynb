{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-20 13:25:43--  https://www.dropbox.com/s/2yjaru8mbkeynwe/dataset%20%283%29.csv?dl=1\n",
      "Resolving www.dropbox.com... 162.125.64.1, 2620:100:6020:1::a27d:4001\n",
      "Connecting to www.dropbox.com|162.125.64.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/dl/2yjaru8mbkeynwe/dataset%20%283%29.csv [following]\n",
      "--2020-05-20 13:25:43--  https://www.dropbox.com/s/dl/2yjaru8mbkeynwe/dataset%20%283%29.csv\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uccd2a1b7789d31309ca5f04670f.dl.dropboxusercontent.com/cd/0/get/A4FqtYhLPIuZzMilSjeu_8jNlFHyJpq6KvMmSpz501aKgJOpjnAL9hbQX1I9ADV7Bpl0mURnAIksuPE6m1jt7ZgfTStlsomhmxyK66cyk4pT1A4WVqPP_EKD_16hbpgipks/file?dl=1# [following]\n",
      "--2020-05-20 13:25:44--  https://uccd2a1b7789d31309ca5f04670f.dl.dropboxusercontent.com/cd/0/get/A4FqtYhLPIuZzMilSjeu_8jNlFHyJpq6KvMmSpz501aKgJOpjnAL9hbQX1I9ADV7Bpl0mURnAIksuPE6m1jt7ZgfTStlsomhmxyK66cyk4pT1A4WVqPP_EKD_16hbpgipks/file?dl=1\n",
      "Resolving uccd2a1b7789d31309ca5f04670f.dl.dropboxusercontent.com... 162.125.6.6, 2620:100:601c:6::a27d:606\n",
      "Connecting to uccd2a1b7789d31309ca5f04670f.dl.dropboxusercontent.com|162.125.6.6|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2555694 (2.4M) [application/binary]\n",
      "Saving to: ‘dataset.csv’\n",
      "\n",
      "dataset.csv         100%[===================>]   2.44M  --.-KB/s    in 0.02s   \n",
      "\n",
      "2020-05-20 13:25:44 (133 MB/s) - ‘dataset.csv’ saved [2555694/2555694]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Загрузка файла с табличными данными\n",
    "!wget https://www.dropbox.com/s/2yjaru8mbkeynwe/dataset%20%283%29.csv?dl=1 -O dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Импортирование необходимых библиотек для обработки и чтение файла с табличными данными\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import torch\n",
    "import zipfile\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "dataset = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Обработка временных показателей даты загрузки видео на видеохостинг и даты создания канала.\n",
    "dataset['Time_cur'] = pd.to_datetime(dataset['published_at'])\n",
    "dataset['published_hour_cur'] = dataset['Time_cur'].dt.hour\n",
    "dataset['Weekday_cur'] = dataset['Time_cur'].dt.weekday\n",
    "diff = dt.datetime.now() - dataset['Time_cur']\n",
    "dataset['Published_cur_time'] = (dt.datetime.now() - dataset['Time_cur']).dt.total_seconds()\n",
    "dataset  = dataset.drop(['Time_cur', 'published_at'], axis=1)\n",
    "dataset['channel_founded'] = pd.to_datetime(dataset['channel_founded'])\n",
    "diff = dt.datetime.now() - dataset['channel_founded']\n",
    "dataset['channel_founded'] = diff.dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py37_default/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Деление видео на классы популярных и непопулярных\n",
    "dataset['popular'] = 0.0\n",
    "dataset['popular'][dataset.view_count_new.gt(dataset.view_count_new.quantile(0.8))] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обработка категориальных признаков(день недели и час публикации видео) при помощи one hot encoder.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "dataset = pd.get_dummies(dataset, columns=['published_hour_cur', 'Weekday_cur'], prefix=[\"published_hour\", \"Weekday_cur\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Приминение нормализации к вещественным признакам, которые используются для классификации видео.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "dataset_norm = dataset[['channel_founded', 'subcriber_count', 'view_count', 'like_count', 'dislike_count', 'favorite_count', 'comment_count',  'Published_cur_time' ]]\n",
    "dataset_norm  = dataset_norm.values\n",
    "scaler = StandardScaler()\n",
    "dataset_norm = scaler.fit_transform(dataset_norm)\n",
    "dataset[['channel_founded', 'subcriber_count', 'view_count', 'like_count', 'dislike_count', 'favorite_count', 'comment_count',  'Published_cur_time' ]] = dataset_norm \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание класса data.dataset, который необходим для загрузки данных в модель MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "class numerical(data.Dataset):\n",
    "    def __init__(self, numerical, cat, y):\n",
    "        \"Initialization\"\n",
    "        self.numerical = numerical\n",
    "        self.cat = cat\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def read_images_folder(self, folder):\n",
    "        frames_vid = os.listdir(os.path.join(self.data_path_frame, folder))\n",
    "        def myFunc(e):\n",
    "            return int(e.split('.')[0])\n",
    "        out = []\n",
    "        frames_vid.sort(reverse=False, key=myFunc)\n",
    "        for pic in frames_vid:\n",
    "            image = Image.open(os.path.join(self.data_path_frame, folder, pic))\n",
    "            if self.transform_frames:\n",
    "                image = self.transform_frames(image)\n",
    "            out.append(image)\n",
    "        out = torch.stack(out, dim = 0)\n",
    "        return out\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        num = torch.from_numpy(self.numerical[index])\n",
    "        cat = torch.from_numpy(self.cat[index])\n",
    "        y = list(self.y)[index]\n",
    "        return num, cat, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Разбиение набора данных на обучающую, валидационную и тестовую выборку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = int(0.8*len(dataset))\n",
    "val_ids = int(0.8*len(dataset)) + int(0.1*len(dataset))\n",
    "train = dataset[:train_ids]\n",
    "val = dataset[train_ids:val_ids]\n",
    "test = dataset[val_ids:len(dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numerical = train[['channel_founded', 'subcriber_count', 'view_count', 'like_count','dislike_count', 'comment_count', 'Published_cur_time']].values\n",
    "train_categorical = train[['published_hour_0', 'published_hour_1', 'published_hour_2',\n",
    "       'published_hour_3', 'published_hour_4', 'published_hour_5',\n",
    "       'published_hour_6', 'published_hour_7', 'published_hour_8',\n",
    "       'published_hour_9', 'published_hour_10', 'published_hour_11',\n",
    "       'published_hour_12', 'published_hour_13', 'published_hour_14',\n",
    "       'published_hour_15', 'published_hour_16', 'published_hour_17',\n",
    "       'published_hour_18', 'published_hour_19', 'published_hour_20',\n",
    "       'published_hour_21', 'published_hour_22', 'published_hour_23',\n",
    "       'Weekday_cur_0', 'Weekday_cur_1', 'Weekday_cur_2', 'Weekday_cur_3',\n",
    "       'Weekday_cur_4', 'Weekday_cur_5', 'Weekday_cur_6']].values\n",
    "train_target = train[['popular']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_numerical = val[['channel_founded', 'subcriber_count', 'view_count', 'like_count','dislike_count', 'comment_count', 'Published_cur_time']].values\n",
    "val_categorical = val[['published_hour_0', 'published_hour_1', 'published_hour_2',\n",
    "       'published_hour_3', 'published_hour_4', 'published_hour_5',\n",
    "       'published_hour_6', 'published_hour_7', 'published_hour_8',\n",
    "       'published_hour_9', 'published_hour_10', 'published_hour_11',\n",
    "       'published_hour_12', 'published_hour_13', 'published_hour_14',\n",
    "       'published_hour_15', 'published_hour_16', 'published_hour_17',\n",
    "       'published_hour_18', 'published_hour_19', 'published_hour_20',\n",
    "       'published_hour_21', 'published_hour_22', 'published_hour_23',\n",
    "       'Weekday_cur_0', 'Weekday_cur_1', 'Weekday_cur_2', 'Weekday_cur_3',\n",
    "       'Weekday_cur_4', 'Weekday_cur_5', 'Weekday_cur_6']].values\n",
    "val_target = val[['popular']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_numerical = test[['channel_founded', 'subcriber_count', 'view_count', 'like_count','dislike_count', 'comment_count', 'Published_cur_time']].values\n",
    "test_categorical = test[['published_hour_0', 'published_hour_1', 'published_hour_2',\n",
    "       'published_hour_3', 'published_hour_4', 'published_hour_5',\n",
    "       'published_hour_6', 'published_hour_7', 'published_hour_8',\n",
    "       'published_hour_9', 'published_hour_10', 'published_hour_11',\n",
    "       'published_hour_12', 'published_hour_13', 'published_hour_14',\n",
    "       'published_hour_15', 'published_hour_16', 'published_hour_17',\n",
    "       'published_hour_18', 'published_hour_19', 'published_hour_20',\n",
    "       'published_hour_21', 'published_hour_22', 'published_hour_23',\n",
    "       'Weekday_cur_0', 'Weekday_cur_1', 'Weekday_cur_2', 'Weekday_cur_3',\n",
    "       'Weekday_cur_4', 'Weekday_cur_5', 'Weekday_cur_6']].values\n",
    "test_target = test[['popular']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = numerical(train_numerical, train_categorical, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val = numerical(val_numerical, val_categorical, val_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Инициализация data.DataLoader, который позволяет передавать данные по батчам в нейронную сеть.\n",
    "train_dl = data.DataLoader(dataset_train, batch_size = 400)\n",
    "val_dl = data.DataLoader(dataset_val, batch_size = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Класс нейронной сети\n",
    "class numerical_categ(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.numer_1 = nn.Linear(7, 16)\n",
    "        self.funcnumer_1 = nn.LeakyReLU(0.1)\n",
    "        self.numer_2 = nn.Linear(16, 32)\n",
    "        \n",
    "        self.categ_1 = nn.Linear(31, 32)\n",
    "        self.funccateg_1 = nn.LeakyReLU(0.1)\n",
    "        self.categ_2 = nn.Linear(32, 32)\n",
    "        \n",
    "        self.comb1 = nn.Linear(64, 32)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.funccateg_1 = nn.LeakyReLU(0.1)\n",
    "        self.comb2 = nn.Linear(32, 1)\n",
    "        \n",
    "    def forward(self, categ, numerical):\n",
    "        num = self.numer_1(numerical)\n",
    "        num = self.funcnumer_1(num)\n",
    "        num = self.numer_2(num)\n",
    "        \n",
    "        categ = self.categ_1(categ)\n",
    "        categ = self.funccateg_1(categ)\n",
    "        categ = self.categ_2(categ)\n",
    "        \n",
    "        comb = torch.cat([categ, num], axis = -1)\n",
    "        comb = self.comb1(comb)\n",
    "        comb = self.dropout(comb)\n",
    "        comb = self.funccateg_1(comb)\n",
    "        comb = self.comb2(comb)\n",
    "        return comb\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подсчет точности предсказаний\n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    acc = acc.item()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.DataParallel(numerical_categ().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функция обучения модели\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "train_log = []\n",
    "val_log = []\n",
    "import matplotlib.pyplot as plt\n",
    "def train_model(model, epoch, lr):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    opt_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5)\n",
    "    opt_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'max', patience =  1, factor=0.1)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    writer = SummaryWriter()\n",
    "    for i in range(epoch):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        train_log_epoch = []\n",
    "        train_val_epoch = []\n",
    "        num_batch = 0\n",
    "        total_predicts = []\n",
    "        total_corrects = []\n",
    "        \n",
    "        for elem in train_dl:\n",
    "            optimizer.zero_grad()\n",
    "            num = elem[0].float().cuda()\n",
    "            cat = elem[1].float().cuda()\n",
    "            y = elem[2].float().cuda()\n",
    "            y_pred = model(cat, num)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss = loss.item()\n",
    "            acc = binary_acc(y_pred, y)\n",
    "            train_log_epoch.append(acc)\n",
    "            y_pred_sig = torch.sigmoid(y_pred)\n",
    "            y_pred_sig = y_pred_sig.flatten().tolist()\n",
    "            total_predicts.extend(y_pred_sig)\n",
    "            total_corrects.extend(y.flatten().tolist())\n",
    "        train_log.extend(train_log_epoch)\n",
    "        acc = np.mean(train_log_epoch)\n",
    "        binary_preds = np.array(total_predicts)>0.5\n",
    "        binary_preds = binary_preds.astype(int)\n",
    "        binary_preds = list(binary_preds)\n",
    "        print(len(binary_preds))\n",
    "        print(len(total_corrects))\n",
    "        torch.save(model.state_dict(), 'checkpoint/model_numer'+str(i)+'.pth')\n",
    "        prec = precision_score(total_corrects, binary_preds)\n",
    "        rec = recall_score(total_corrects, binary_preds)\n",
    "        roc_auc = roc_auc_score(total_corrects, total_predicts)\n",
    "        writer.add_scalar('precision_train', prec, i)\n",
    "        writer.add_scalar('recall_train', rec, i)\n",
    "        writer.add_scalar('roc_auc_train', roc_auc, i)\n",
    "        writer.add_scalar('accuracy_train', np.mean(train_log_epoch), i)\n",
    "        print(prec)\n",
    "        print(rec)\n",
    "        print(roc_auc)\n",
    "        print(acc)\n",
    "        \n",
    "        print('test')\n",
    "        prec_test, rec_test, roc_auc_test, acc_test = val(model, val_dl)\n",
    "        writer.add_scalar('precision_test', prec_test, i)\n",
    "        writer.add_scalar('recall_test', rec_test, i)\n",
    "        writer.add_scalar('roc_auc_test', roc_auc_test, i)\n",
    "        writer.add_scalar('accuracy_test', acc_test, i)\n",
    "        opt_scheduler.step(acc_test)\n",
    "        print(prec_test)\n",
    "        print(rec_test)\n",
    "        print(roc_auc_test)\n",
    "        print(acc_test)\n",
    "        writer.close()\n",
    "    return model\n",
    "\n",
    "\n",
    "def val(model, valid_dl):\n",
    "    model.eval()\n",
    "    sum_loss = 0.0\n",
    "    val_epoch = []\n",
    "    total_preds = []\n",
    "    total_corrects = []\n",
    "    for elem in valid_dl:\n",
    "        num = elem[0].float().cuda()\n",
    "        cat = elem[1].float().cuda()\n",
    "        y = elem[2].float().cuda()\n",
    "        y_pred = model(cat, num)\n",
    "        y_pred_sig = torch.sigmoid(y_pred)\n",
    "        y_pred_sig = y_pred_sig.flatten().tolist()\n",
    "        total_preds.extend(y_pred_sig)\n",
    "        total_corrects.extend(y.flatten().tolist())\n",
    "        acc = binary_acc(y_pred, y)\n",
    "        val_epoch.append(acc)\n",
    "    binary_preds = np.array(total_preds)>0.5\n",
    "    binary_preds = binary_preds.astype(int)\n",
    "    binary_preds = list(binary_preds)\n",
    "    prec = precision_score(total_corrects, binary_preds)\n",
    "    rec = recall_score(total_corrects, binary_preds)\n",
    "    roc_auc = roc_auc_score(total_corrects, total_preds)\n",
    "    acc = np.mean(val_epoch)\n",
    "    return prec, rec, roc_auc, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8278\n",
      "8278\n",
      "0.6424581005586593\n",
      "0.06894484412470024\n",
      "0.5349415172854153\n",
      "80.23809523809524\n",
      "test\n",
      "0.5555555555555556\n",
      "0.189873417721519\n",
      "0.7786424831514805\n",
      "79.33333333333333\n",
      "8278\n",
      "8278\n",
      "0.7\n",
      "0.16786570743405277\n",
      "0.7294647489270307\n",
      "81.52380952380952\n",
      "test\n",
      "0.6016949152542372\n",
      "0.29957805907172996\n",
      "0.798373648015501\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.7073509015256588\n",
      "0.3057553956834532\n",
      "0.7832059465891734\n",
      "83.28571428571429\n",
      "test\n",
      "0.5684210526315789\n",
      "0.45569620253164556\n",
      "0.8149706970760604\n",
      "80.33333333333333\n",
      "8278\n",
      "8278\n",
      "0.6905241935483871\n",
      "0.4106714628297362\n",
      "0.8075335495597471\n",
      "84.38095238095238\n",
      "test\n",
      "0.5894736842105263\n",
      "0.47257383966244726\n",
      "0.8319224518103225\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.6994219653179191\n",
      "0.4352517985611511\n",
      "0.8349369369859634\n",
      "84.52380952380952\n",
      "test\n",
      "0.5869565217391305\n",
      "0.45569620253164556\n",
      "0.8332989215888696\n",
      "80.33333333333333\n",
      "8278\n",
      "8278\n",
      "0.6999012833168805\n",
      "0.4250599520383693\n",
      "0.8375857559035979\n",
      "84.57142857142857\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8343365680373128\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.7019607843137254\n",
      "0.4292565947242206\n",
      "0.8384733363082604\n",
      "84.76190476190476\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8344000974117074\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.7052529182879378\n",
      "0.43465227817745805\n",
      "0.8402885407256645\n",
      "84.61904761904762\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8343948032971746\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.6933728981206726\n",
      "0.42026378896882494\n",
      "0.8404016877269742\n",
      "84.33333333333333\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8343842150681089\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.7063953488372093\n",
      "0.4370503597122302\n",
      "0.8434209667062115\n",
      "84.9047619047619\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8343948032971745\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.703921568627451\n",
      "0.4304556354916067\n",
      "0.8420478745596562\n",
      "84.71428571428571\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8343948032971745\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.6996124031007752\n",
      "0.43285371702637887\n",
      "0.8394064929599436\n",
      "84.61904761904762\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8343842150681087\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.7012487992315082\n",
      "0.43764988009592326\n",
      "0.8401979324256178\n",
      "84.85714285714286\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8343842150681087\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.6967370441458733\n",
      "0.4352517985611511\n",
      "0.8393651795658783\n",
      "84.61904761904762\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8343842150681087\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.695274831243973\n",
      "0.43225419664268583\n",
      "0.8398097860592012\n",
      "84.57142857142857\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8343842150681087\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.7042532146389713\n",
      "0.42685851318944845\n",
      "0.8394829975656389\n",
      "84.76190476190476\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8343842150681087\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.7003891050583657\n",
      "0.4316546762589928\n",
      "0.8385317011141465\n",
      "84.66666666666667\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8343842150681087\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.7064579256360078\n",
      "0.43285371702637887\n",
      "0.8389816134989135\n",
      "84.85714285714286\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8343842150681087\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.6940948693126815\n",
      "0.42985611510791366\n",
      "0.841172719917863\n",
      "84.57142857142857\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8343842150681087\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.6941176470588235\n",
      "0.4244604316546763\n",
      "0.8408578129931759\n",
      "84.47619047619048\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8343842150681087\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.702729044834308\n",
      "0.43225419664268583\n",
      "0.8422274585777672\n",
      "84.71428571428571\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8343842150681087\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.7028073572120038\n",
      "0.4352517985611511\n",
      "0.8398462470568175\n",
      "84.80952380952381\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8343842150681087\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.7080867850098619\n",
      "0.4304556354916067\n",
      "0.8409227534764927\n",
      "84.71428571428571\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8343842150681087\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.6982591876208898\n",
      "0.43285371702637887\n",
      "0.8431260135613144\n",
      "84.66666666666667\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8343842150681087\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.7034482758620689\n",
      "0.42805755395683454\n",
      "0.840026148521425\n",
      "84.71428571428571\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8343842150681087\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.6985507246376812\n",
      "0.43345323741007197\n",
      "0.8435483534503714\n",
      "84.76190476190476\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8343842150681087\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.7001953125\n",
      "0.42985611510791366\n",
      "0.8414245910382133\n",
      "84.66666666666667\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8343842150681087\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.6943907156673114\n",
      "0.4304556354916067\n",
      "0.8407188621266376\n",
      "84.61904761904762\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8343842150681087\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.6989141164856861\n",
      "0.4244604316546763\n",
      "0.842704943458244\n",
      "84.71428571428571\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8343842150681087\n",
      "80.66666666666667\n",
      "8278\n",
      "8278\n",
      "0.7016520894071915\n",
      "0.43285371702637887\n",
      "0.8436779169705083\n",
      "84.80952380952381\n",
      "test\n",
      "0.5919540229885057\n",
      "0.4345991561181435\n",
      "0.8343842150681087\n",
      "80.66666666666667\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, 30, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = numerical(test_numerical, test_categorical, test_target)\n",
    "test_dl = data.DataLoader(dataset_test, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec, rec, roc_auc, acc = val(model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5161290322580645 0.3878787878787879 0.8075983717774763 84.81818181818181\n"
     ]
    }
   ],
   "source": [
    "print(prec, rec, roc_auc, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test, thresh):\n",
    "    y_pred_tag = torch.gt(torch.sigmoid(y_pred), thresh).int()\n",
    "    y_pred_tag = y_pred_tag\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    print(correct_results_sum)\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = acc.item()\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "model.eval()\n",
    "sum_loss = 0.0\n",
    "val_epoch = []\n",
    "total_preds = []\n",
    "total_corrects = []\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "thresholds = np.arange(0, 1, 0.05)\n",
    "for thresh in tqdm(thresholds):\n",
    "    for elem in val_dl:\n",
    "        num = elem[0].float().cuda()\n",
    "        cat = elem[1].float().cuda()\n",
    "        y = elem[2].float().cuda()\n",
    "        y_pred = model(cat, num)\n",
    "        y_pred_sig = torch.sigmoid(y_pred)\n",
    "        y_pred_sig = y_pred_sig.flatten().tolist()\n",
    "        total_preds.extend(y_pred_sig)\n",
    "        total_corrects.extend(y.flatten().tolist())\n",
    "        acc = binary_acc(y_pred, y, thresh)\n",
    "    val_epoch.append(acc)\n",
    "    binary_preds = np.array(total_preds)>thresh\n",
    "    binary_preds = binary_preds.astype(int)\n",
    "    binary_preds = list(binary_preds)\n",
    "    print('test')\n",
    "    prec = precision_score(total_corrects, binary_preds)\n",
    "    rec = recall_score(total_corrects, binary_preds)\n",
    "    roc_auc = roc_auc_score(total_corrects, total_preds)\n",
    "    acc = np.mean(val_epoch)\n",
    "    print('thresh', thresh, 'prec', prec, 'rec', rec, 'roc_auc', roc_auc, 'acc', acc  )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = precision_recall_curve(total_corrects, total_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = precision_recall_curve(total_corrects, total_preds)\n",
    "import matplotlib.pyplot as plt\n",
    "precision, recall, thrs = out\n",
    "plt.plot(recall, precision)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numerical = train[['channel_founded', 'subcriber_count', 'view_count', 'like_count','dislike_count', 'comment_count', 'Published_cur_time']]\n",
    "train_categorical = train[['published_hour_0', 'published_hour_1', 'published_hour_2',\n",
    "       'published_hour_3', 'published_hour_4', 'published_hour_5',\n",
    "       'published_hour_6', 'published_hour_7', 'published_hour_8',\n",
    "       'published_hour_9', 'published_hour_10', 'published_hour_11',\n",
    "       'published_hour_12', 'published_hour_13', 'published_hour_14',\n",
    "       'published_hour_15', 'published_hour_16', 'published_hour_17',\n",
    "       'published_hour_18', 'published_hour_19', 'published_hour_20',\n",
    "       'published_hour_21', 'published_hour_22', 'published_hour_23',\n",
    "       'Weekday_cur_0', 'Weekday_cur_1', 'Weekday_cur_2', 'Weekday_cur_3',\n",
    "       'Weekday_cur_4', 'Weekday_cur_5', 'Weekday_cur_6']]\n",
    "train_target = train[['popular']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numerical = train_numerical.rename(columns={\"view_count\": \"view_count_prev\", \"like_count\": \"like_count_prev\", \"dislike_count\":\"dislike_count_prev\", \"comment_count\":\"comment_count_prev\" })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_boost = [train_numerical, train_categorical]\n",
    "train_boost = pd.concat(train_boost, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf =  GradientBoostingClassifier(random_state=0)\n",
    "clf.fit(train_boost, train_target.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_numerical = val[['channel_founded', 'subcriber_count', 'view_count', 'like_count','dislike_count', 'comment_count', 'Published_cur_time']]\n",
    "val_categorical = val[['published_hour_0', 'published_hour_1', 'published_hour_2',\n",
    "       'published_hour_3', 'published_hour_4', 'published_hour_5',\n",
    "       'published_hour_6', 'published_hour_7', 'published_hour_8',\n",
    "       'published_hour_9', 'published_hour_10', 'published_hour_11',\n",
    "       'published_hour_12', 'published_hour_13', 'published_hour_14',\n",
    "       'published_hour_15', 'published_hour_16', 'published_hour_17',\n",
    "       'published_hour_18', 'published_hour_19', 'published_hour_20',\n",
    "       'published_hour_21', 'published_hour_22', 'published_hour_23',\n",
    "       'Weekday_cur_0', 'Weekday_cur_1', 'Weekday_cur_2', 'Weekday_cur_3',\n",
    "       'Weekday_cur_4', 'Weekday_cur_5', 'Weekday_cur_6']]\n",
    "val_target = val[['popular']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_numerical = val_numerical.rename(columns={\"view_count\": \"view_count_prev\", \"like_count\": \"like_count_prev\", \"dislike_count\":\"dislike_count_prev\", \"comment_count\":\"comment_count_prev\" })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_boost = [val_numerical, val_categorical]\n",
    "val_boost = pd.concat(val_boost, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance[sorted_idx][10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.inspection import permutation_importance\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(10) + .5\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.barh(pos, feature_importance[sorted_idx][-10:], align='center')\n",
    "plt.yticks(pos, np.array(train_boost.columns)[sorted_idx][-10:])\n",
    "plt.title('Feature Importance')\n",
    "\n",
    "result = permutation_importance(clf, val_boost, val_target, n_repeats=10,\n",
    "                                random_state=42, n_jobs=2)\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(result.importances[sorted_idx][-10:].T,\n",
    "            vert=False, labels=np.array(val_boost.columns)[sorted_idx][-10:])\n",
    "plt.title(\"Permutation Importance (test set)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict_proba(val_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = precision_score(val_target, list(np.round(res)))\n",
    "rec = recall_score(val_target, list(np.round(res)))\n",
    "roc_auc = roc_auc_score(val_target, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_numerical =  test[['channel_founded', 'subcriber_count', 'view_count', 'like_count','dislike_count', 'comment_count', 'Published_cur_time']]\n",
    "test_categorical = test[['published_hour_0', 'published_hour_1', 'published_hour_2',\n",
    "       'published_hour_3', 'published_hour_4', 'published_hour_5',\n",
    "       'published_hour_6', 'published_hour_7', 'published_hour_8',\n",
    "       'published_hour_9', 'published_hour_10', 'published_hour_11',\n",
    "       'published_hour_12', 'published_hour_13', 'published_hour_14',\n",
    "       'published_hour_15', 'published_hour_16', 'published_hour_17',\n",
    "       'published_hour_18', 'published_hour_19', 'published_hour_20',\n",
    "       'published_hour_21', 'published_hour_22', 'published_hour_23',\n",
    "       'Weekday_cur_0', 'Weekday_cur_1', 'Weekday_cur_2', 'Weekday_cur_3',\n",
    "       'Weekday_cur_4', 'Weekday_cur_5', 'Weekday_cur_6']]\n",
    "test_target = test[['popular']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_boost = [test_numerical, test_categorical]\n",
    "test_boost = pd.concat(test_boost, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr = 0.1\n",
    "#n_est = 100\n",
    "#max_depth = 3\n",
    "clf =  GradientBoostingClassifier(learning_rate=0.1, n_estimators=100, max_depth=3)\n",
    "clf.fit(train_boost, train_target.values.ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict_proba(val_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test, thresh):\n",
    "    y_pred_tag = (y_pred>thresh).astype(int)\n",
    "    print(y_pred_tag.shape, y_test.shape)\n",
    "    correct_results_sum = (y_pred_tag == y_test.squeeze()).sum()\n",
    "    print(correct_results_sum)\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = acc.item()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "prec = precision_score(val_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "rec = recall_score(val_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "roc_auc = roc_auc_score(val_target.squeeze().astype(int), np.round(res[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prec)\n",
    "print(rec)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_acc(np.round(res[:,1]), val_target.squeeze().astype(int), 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict_proba(test_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "prec = precision_score(test_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "rec = recall_score(test_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "roc_auc = roc_auc_score(test_target.squeeze().astype(int), np.round(res[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prec)\n",
    "print(rec)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_acc(np.round(res[:,1]), test_target.squeeze().astype(int), 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr = 0.1\n",
    "#n_est = 1000\n",
    "#max_depth = 3\n",
    "clf =  GradientBoostingClassifier(learning_rate=0.1, n_estimators=1000, max_depth=3)\n",
    "clf.fit(train_boost, train_target.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict_proba(val_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = precision_score(val_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "rec = recall_score(val_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "roc_auc = roc_auc_score(val_target.squeeze().astype(int), np.round(res[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prec)\n",
    "print(rec)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_acc(np.round(res[:,1]), val_target.squeeze().astype(int), 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict_proba(test_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = precision_score(test_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "rec = recall_score(test_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "roc_auc = roc_auc_score(test_target.squeeze().astype(int), np.round(res[:,1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prec)\n",
    "print(rec)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_acc(np.round(res[:,1]), test_target.squeeze().astype(int), 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr = 0.1\n",
    "#n_est = 100\n",
    "#max_depth = 3\n",
    "clf =  GradientBoostingClassifier(learning_rate=0.1, n_estimators=10000, max_depth=3)\n",
    "clf.fit(train_boost, train_target.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict_proba(val_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = precision_score(val_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "rec = recall_score(val_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "roc_auc = roc_auc_score(val_target.squeeze().astype(int), np.round(res[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prec)\n",
    "print(rec)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_acc(np.round(res[:,1]), val_target.squeeze().astype(int), 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict_proba(test_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = precision_score(test_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "rec = recall_score(test_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "roc_auc = roc_auc_score(test_target.squeeze().astype(int), np.round(res[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prec)\n",
    "print(rec)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_acc(np.round(res[:,1]), test_target.squeeze().astype(int), 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf =  GradientBoostingClassifier(learning_rate=0.01, n_estimators=100, max_depth=3)\n",
    "clf.fit(train_boost, train_target.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict_proba(val_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = precision_score(val_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "rec = recall_score(val_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "roc_auc = roc_auc_score(val_target.squeeze().astype(int), np.round(res[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prec)\n",
    "print(rec)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_acc(np.round(res[:,1]), val_target.squeeze().astype(int), 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict_proba(test_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = precision_score(test_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "rec = recall_score(test_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "roc_auc = roc_auc_score(test_target.squeeze().astype(int), np.round(res[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prec)\n",
    "print(rec)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_acc(np.round(res[:,1]), test_target.squeeze().astype(int), 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf =  GradientBoostingClassifier(learning_rate=0.001, n_estimators=100, max_depth=3)\n",
    "clf.fit(train_boost, train_target.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict_proba(val_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = precision_score(val_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "rec = recall_score(val_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "roc_auc = roc_auc_score(val_target.squeeze().astype(int), np.round(res[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prec)\n",
    "print(rec)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_acc(np.round(res[:,1]), val_target.squeeze().astype(int), 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict_proba(test_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = precision_score(test_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "rec = recall_score(test_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "roc_auc = roc_auc_score(test_target.squeeze().astype(int), np.round(res[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prec)\n",
    "print(rec)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_acc(np.round(res[:,1]), test_target.squeeze().astype(int), 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf =  GradientBoostingClassifier(learning_rate=0.1, n_estimators=100, max_depth=4)\n",
    "clf.fit(train_boost, train_target.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict_proba(val_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = precision_score(val_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "rec = recall_score(val_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "roc_auc = roc_auc_score(val_target.squeeze().astype(int), np.round(res[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prec)\n",
    "print(rec)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_acc(np.round(res[:,1]), val_target.squeeze().astype(int), 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict_proba(test_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = precision_score(test_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "rec = recall_score(test_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "roc_auc = roc_auc_score(test_target.squeeze().astype(int), np.round(res[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prec)\n",
    "print(rec)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_acc(np.round(res[:,1]), test_target.squeeze().astype(int), 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf =  GradientBoostingClassifier(learning_rate=0.1, n_estimators=100, max_depth=1)\n",
    "clf.fit(train_boost, train_target.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict_proba(val_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = precision_score(val_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "rec = recall_score(val_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "roc_auc = roc_auc_score(val_target.squeeze().astype(int), np.round(res[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prec)\n",
    "print(rec)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_acc(np.round(res[:,1]), val_target.squeeze().astype(int), 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict_proba(test_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = precision_score(test_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "rec = recall_score(test_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "roc_auc = roc_auc_score(test_target.squeeze().astype(int), np.round(res[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prec)\n",
    "print(rec)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_acc(np.round(res[:,1]), test_target.squeeze().astype(int), 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf =  GradientBoostingClassifier(learning_rate=0.1, n_estimators=100, max_depth=2)\n",
    "clf.fit(train_boost, train_target.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict_proba(val_boost)\n",
    "prec = precision_score(val_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "rec = recall_score(val_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "roc_auc = roc_auc_score(val_target.squeeze().astype(int), np.round(res[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prec)\n",
    "print(rec)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_acc(np.round(res[:,1]), val_target.squeeze().astype(int), 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict_proba(test_boost)\n",
    "prec = precision_score(test_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "rec = recall_score(test_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "roc_auc = roc_auc_score(test_target.squeeze().astype(int), np.round(res[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prec)\n",
    "print(rec)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_acc(np.round(res[:,1]), test_target.squeeze().astype(int), 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf =  GradientBoostingClassifier(learning_rate=0.1, n_estimators=100, max_depth=5)\n",
    "clf.fit(train_boost, train_target.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict_proba(val_boost)\n",
    "prec = precision_score(val_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "rec = recall_score(val_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "roc_auc = roc_auc_score(val_target.squeeze().astype(int), np.round(res[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prec)\n",
    "print(rec)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_acc(np.round(res[:,1]), val_target.squeeze().astype(int), 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict_proba(test_boost)\n",
    "prec = precision_score(test_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "rec = recall_score(test_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "roc_auc = roc_auc_score(test_target.squeeze().astype(int), np.round(res[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prec)\n",
    "print(rec)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_acc(np.round(res[:,1]), test_target.squeeze().astype(int), 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf =  GradientBoostingClassifier(learning_rate=0.01, n_estimators=10000, max_depth=1)\n",
    "clf.fit(train_boost, train_target.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict_proba(val_boost)\n",
    "prec = precision_score(val_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "rec = recall_score(val_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "roc_auc = roc_auc_score(val_target.squeeze().astype(int), np.round(res[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prec)\n",
    "print(rec)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_acc(np.round(res[:,1]), val_target.squeeze().astype(int), 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict_proba(test_boost)\n",
    "prec = precision_score(test_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "rec = recall_score(test_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "roc_auc = roc_auc_score(test_target.squeeze().astype(int), np.round(res[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prec)\n",
    "print(rec)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_acc(np.round(res[:,1]), test_target.squeeze().astype(int), 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf =  GradientBoostingClassifier(learning_rate=0.01, n_estimators=10000, max_depth=2)\n",
    "clf.fit(train_boost, train_target.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict_proba(val_boost)\n",
    "prec = precision_score(val_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "rec = recall_score(val_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "roc_auc = roc_auc_score(val_target.squeeze().astype(int), np.round(res[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prec)\n",
    "print(rec)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_acc(np.round(res[:,1]), val_target.squeeze().astype(int), 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = clf.predict_proba(test_boost)\n",
    "prec = precision_score(test_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "rec = recall_score(test_target.squeeze().astype(int), np.round(res[:,1]))\n",
    "roc_auc = roc_auc_score(test_target.squeeze().astype(int), np.round(res[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prec)\n",
    "print(rec)\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(binary_acc(np.round(res[:,1]), test_target.squeeze().astype(int), 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "out = precision_recall_curve(test_target, res)\n",
    "precision, recall, thrs = out\n",
    "plt.plot(recall, precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0, 1, 0.05)\n",
    "for thresh in tqdm(thresholds):\n",
    "    y_bin = res > thresh\n",
    "    y_bin = y_bin.astype(int)\n",
    "\n",
    "    acc = binary_acc(res, val_target, thresh)\n",
    "    prec = precision_score(val_target, y_bin)\n",
    "    rec = recall_score(val_target, y_bin)\n",
    "    roc_auc = roc_auc_score(val_target, res)\n",
    "    print('thresh', thresh, 'prec', prec, 'rec', rec, 'roc_auc', roc_auc, 'acc', acc  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
