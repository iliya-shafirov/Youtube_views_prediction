{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/zp8m6v69kvfk2os/images-2.zip?dl=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/wq4f8m959yloai3/video_frames_long.zip?dl=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install pytorch==1.2.0 torchvision==0.4.0 cudatoolkit=10.0 -c pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Выше ссылки для скачивания архивов с раскадровкой и изображениями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import torch\n",
    "import zipfile\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "dataset = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Time_cur'] = pd.to_datetime(dataset['published_at'])\n",
    "dataset['published_hour_cur'] = dataset['Time_cur'].dt.hour\n",
    "dataset['Weekday_cur'] = dataset['Time_cur'].dt.weekday\n",
    "diff = dt.datetime.now() - dataset['Time_cur']\n",
    "dataset['Published_cur_time'] = (dt.datetime.now() - dataset['Time_cur']).dt.total_seconds()\n",
    "dataset  = dataset.drop(['Time_cur', 'published_at'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.sort_values(by = 'Published_cur_time',   ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['channel_founded'] = pd.to_datetime(dataset['channel_founded'])\n",
    "diff = dt.datetime.now() - dataset['channel_founded']\n",
    "dataset['channel_founded'] = diff.dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['popular'] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['popular'][dataset.view_count_new.gt(dataset.view_count_new.quantile(0.8))] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Кодировка категориальных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "dataset = pd.get_dummies(dataset, columns=['published_hour_cur', 'Weekday_cur'], prefix=[\"published_hour\", \"Weekday_cur\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Нормирование вещественных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "dataset_norm = dataset[['channel_founded', 'subcriber_count', 'view_count', 'like_count', 'dislike_count', 'favorite_count', 'comment_count',  'Published_cur_time' ]]\n",
    "dataset_norm  = dataset_norm.values\n",
    "scaler = StandardScaler()\n",
    "dataset_norm = scaler.fit_transform(dataset_norm)\n",
    "dataset[['channel_founded', 'subcriber_count', 'view_count', 'like_count', 'dislike_count', 'favorite_count', 'comment_count',  'Published_cur_time' ]] = dataset_norm \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[~dataset.isin([np.nan, np.inf, -np.inf]).any(1)]\n",
    "dataset = dataset.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile(\"videos_11k.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall('videos11_k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_pres = []\n",
    "def video_frames(video_folder, new_folder, snap_full = True):\n",
    "    videos = os.listdir(path = video_folder) \n",
    "    os.mkdir(new_folder)\n",
    "    for elem in videos:\n",
    "        cap = cv2.VideoCapture(video_folder+elem)\n",
    "        fps = int(cap.get(5))\n",
    "        lp = np.linspace(33, fps * 10 , num = 20)\n",
    "        lp = lp.astype('int')\n",
    "        frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        frames = int(frames)\n",
    "        if snap_full == True:  \n",
    "            lp = np.linspace(33, frames-4, num = 20)\n",
    "            lp = lp.astype('int')\n",
    "        os.mkdir(new_folder + elem.split('.')[0])\n",
    "        for ind in lp:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, ind)\n",
    "            ret, frame = cap.read()\n",
    "            if ret == False:\n",
    "                print('hello')\n",
    "                print(elem)\n",
    "                not_pres.append(elem)\n",
    "            else:\n",
    "                cv2.imwrite(new_folder+elem.split('.')[0] + '/'+ str(ind) + '.png', frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_frames(\"videos/videos/\", 'video_frames_normal/', snap_full = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_pres = list(set(not_pres))\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "for elem in not_pres:\n",
    "    adr = elem.split('.')[0]\n",
    "    dirpath = Path('video_frames_normal', adr)\n",
    "    if dirpath.exists() and dirpath.is_dir():\n",
    "        shutil.rmtree(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "dataset['video_names'] = ''\n",
    "i = 0\n",
    "for elem in dataset['video']:\n",
    "    dataset['video_names'][i] = elem.split('.')[0]\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elems = os.listdir('video_frames_long/video_frames_long')\n",
    "dataset = dataset[dataset['video_names'].isin(elems)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применение spacy для токенизирования текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import string\n",
    "tok = spacy.load('en')\n",
    "#Применение regex для удаления  пунктуации\n",
    "def tokenize (text):\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    text = no_specials_string = re.sub('[!#?,.:\";]', ' ', text)\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '\\\\r\\\\t\\\\n]')\n",
    "    nopunct = regex.sub(\" \", text.lower())\n",
    "    tmp = [token.text for token in tok.tokenizer(nopunct)]\n",
    "    return [x for x in tmp if not x.isspace()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    dataset['video_title'][i] = tokenize(dataset['video_title'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2id = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "#Составление словаря и создание уникального идентификатора для каждого слова.\n",
    "from collections import Counter\n",
    "counts_words = Counter()\n",
    "for row in dataset['video_title']:\n",
    "    counts_words.update(row)\n",
    "for word in counts_words:\n",
    "    vocab2id[word] = len(words)\n",
    "    words.append(word)\n",
    "encoded_texts = []\n",
    "dataset['num_words'] = 0\n",
    "for i in range(len(dataset)):\n",
    "    dataset['num_words'][i] = len(dataset['video_title'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Удаление элементов с пустым оглавлением\n",
    "dataset = dataset[dataset['num_words'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Замена слов в заголовках на уникальные идентификаторы\n",
    "max_len_title = max(dataset['num_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Замена слов в заголовках на уникальные идентификаторы\n",
    "for elem in dataset['video_title']:\n",
    "    enc = np.zeros(max_len_title)\n",
    "    smth = np.array([vocab2id.get(word, vocab2id[\"UNK\"]) for word in elem])\n",
    "    enc[:len(smth)] = smth\n",
    "    encoded_texts.append(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_texts = np.array(encoded_texts,  dtype =  int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.Series( (v for v in encoded_texts) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Encoded'] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Скачивание предобученных эмбеддингов GLOVE\n",
    "word_vectors = {}\n",
    "with open(\"glove/glove.twitter.27B.100d.txt\") as f:\n",
    "    for line in f:\n",
    "        arr = line.split()\n",
    "        word_vectors[arr[0]] = np.array([float(x) for x in arr[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cоставление матрицы эмбеддингов для словаря \n",
    "vocab_size = len(counts_words) + 2\n",
    "vocab_to_idx = {}\n",
    "vocab = [\"\", \"UNK\"]\n",
    "W = np.zeros((vocab_size, 100), dtype=\"float32\")\n",
    "W[0] = np.zeros(100, dtype='float32')\n",
    "W[1] = np.random.uniform(-0.25, 0.25, 100)\n",
    "vocab_to_idx[\"UNK\"] = 1\n",
    "i = 2\n",
    "for word in counts_words:\n",
    "    if word in word_vectors:\n",
    "        W[i] = word_vectors[word]\n",
    "    else:\n",
    "        W[i] = np.random.uniform(-0.25,0.25, 100)\n",
    "    vocab_to_idx[word] = i\n",
    "    vocab.append(word)\n",
    "    i += 1   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install --upgrade torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Создание класса data.dataset, который необходим для загрузки данных в модель MLP. \n",
    "from torch.utils import data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "class Video_fin_dataset(data.Dataset):\n",
    "    def __init__(self, numerical, cat, data_path_frame, frame_folders, data_path_image, images, text, y,  transform_frames=None, transform_image = None):\n",
    "        \"Initialization\"\n",
    "        self.data_path_frame = data_path_frame\n",
    "        self.data_path_image = data_path_image\n",
    "        self.frame_folders = frame_folders\n",
    "        self.images = images\n",
    "        self.text = text\n",
    "        self.transform_frames = transform_frames\n",
    "        self.transform_image = transform_image\n",
    "        self.numerical = numerical\n",
    "        self.cat = cat\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def read_images_folder(self, folder):\n",
    "        frames_vid = os.listdir(os.path.join(self.data_path_frame, folder))\n",
    "        def myFunc(e):\n",
    "            return int(e.split('.')[0])\n",
    "        out = []\n",
    "        frames_vid.sort(reverse=False, key=myFunc)\n",
    "        for pic in frames_vid:\n",
    "            image = Image.open(os.path.join(self.data_path_frame, folder, pic))\n",
    "            if self.transform_frames:\n",
    "                image = self.transform_frames(image)\n",
    "            out.append(image)\n",
    "        out = torch.stack(out, dim = 0)\n",
    "        return out\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        encoded_text = torch.from_numpy(self.text[index][0])\n",
    "        length_text = self.text[index][1]\n",
    "        num = torch.from_numpy(self.numerical[index])\n",
    "        cat = torch.from_numpy(self.cat[index])\n",
    "        folder_frames = self.frame_folders[index]\n",
    "        out = self.read_images_folder(folder_frames)\n",
    "        image = self.images[index]\n",
    "        image = Image.open(os.path.join(self.data_path_image, image))\n",
    "        image = self.transform_image(image)\n",
    "        y = list(self.y)[index]\n",
    "        return num, cat, (encoded_text, length_text), out, image, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "val, test = train_test_split(test, test_size=0.5, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numerical = train[['channel_founded', 'subcriber_count', 'view_count', 'like_count','dislike_count', 'comment_count', 'Published_cur_time']].values\n",
    "train_categorical = train[['published_hour_0', 'published_hour_1', 'published_hour_2',\n",
    "       'published_hour_3', 'published_hour_4', 'published_hour_5',\n",
    "       'published_hour_6', 'published_hour_7', 'published_hour_8',\n",
    "       'published_hour_9', 'published_hour_10', 'published_hour_11',\n",
    "       'published_hour_12', 'published_hour_13', 'published_hour_14',\n",
    "       'published_hour_15', 'published_hour_16', 'published_hour_17',\n",
    "       'published_hour_18', 'published_hour_19', 'published_hour_20',\n",
    "       'published_hour_21', 'published_hour_22', 'published_hour_23',\n",
    "       'Weekday_cur_0', 'Weekday_cur_1', 'Weekday_cur_2', 'Weekday_cur_3',\n",
    "       'Weekday_cur_4', 'Weekday_cur_5', 'Weekday_cur_6']].values\n",
    "train_text = train[['Encoded', 'num_words']].values\n",
    "train_img = train['review'].values\n",
    "train_vids = train['video_names'].values\n",
    "train_target = train[['popular']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_numerical = val[['channel_founded', 'subcriber_count', 'view_count', 'like_count','dislike_count', 'comment_count', 'Published_cur_time']].values\n",
    "val_categorical = val[['published_hour_0', 'published_hour_1', 'published_hour_2',\n",
    "       'published_hour_3', 'published_hour_4', 'published_hour_5',\n",
    "       'published_hour_6', 'published_hour_7', 'published_hour_8',\n",
    "       'published_hour_9', 'published_hour_10', 'published_hour_11',\n",
    "       'published_hour_12', 'published_hour_13', 'published_hour_14',\n",
    "       'published_hour_15', 'published_hour_16', 'published_hour_17',\n",
    "       'published_hour_18', 'published_hour_19', 'published_hour_20',\n",
    "       'published_hour_21', 'published_hour_22', 'published_hour_23',\n",
    "       'Weekday_cur_0', 'Weekday_cur_1', 'Weekday_cur_2', 'Weekday_cur_3',\n",
    "       'Weekday_cur_4', 'Weekday_cur_5', 'Weekday_cur_6']].values\n",
    "val_text = val[['Encoded', 'num_words']].values\n",
    "val_img = val['review'].values\n",
    "val_vids = val['video_names'].values\n",
    "val_target = val[['popular']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_frame = 'video_frames_long/video_frames_long/'\n",
    "data_path_image = 'images-2/images-2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "transform_train_vid = transforms.Compose([transforms.Resize([340, 240]),\n",
    "                                transforms.RandomCrop([169, 169]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "transform_test_vid = transforms.Compose([transforms.Resize([169, 169]),  transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train_img = transforms.Compose([transforms.Resize([200, 200]),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "transform_test_img = transforms.Compose([transforms.Resize([256, 256]),  transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = Video_fin_dataset(train_numerical, train_categorical, data_path_frame, train_vids, data_path_image, train_img, train_text, train_target, transform_train_vid, transform_train_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val = Video_fin_dataset(val_numerical, val_categorical, data_path_frame, val_vids, data_path_image, val_img, val_text, val_target, transform_test_vid, transform_test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = data.DataLoader(dataset_train, batch_size = 128)\n",
    "val_dl = data.DataLoader(dataset_val, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "class BiLSTM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers,  glove_weights):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim,\n",
    "                            hidden_size = 128,\n",
    "                            bidirectional = True,\n",
    "                            num_layers = num_layers,\n",
    "                            dropout = 0.4,\n",
    "                            batch_first=True)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.func1 = nn.LeakyReLU(0.1)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        \n",
    "\n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x_pack = pack_padded_sequence(x, l, batch_first=True, enforce_sorted=False)\n",
    "        out_st, (ht, ct) = self.lstm(x_pack)\n",
    "        ht = torch.cat((ht[-2,:,:], ht[-1,:,:]),  dim = 1)\n",
    "        out = self.fc1(ht)\n",
    "        out = self.func1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class image(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        in_feats = resnet.fc.in_features\n",
    "        print(in_feats)\n",
    "        base_mod = list(resnet.children())[:-1]\n",
    "        self.base_mod = nn.Sequential(*base_mod)\n",
    "        for child in self.base_mod.children():\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.out1 = nn.Linear(in_feats, 256)\n",
    "        self.func1 = nn.LeakyReLU(0.1)\n",
    "        self.out2 = nn.Linear(256, 32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x  = self.base_mod(x)\n",
    "        print(x.shape)\n",
    "        x =  x.view(-1, 2048)\n",
    "        x = self.out1(x)\n",
    "        x = self.func1(x)\n",
    "        return self.out2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class video_analysis(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(video_analysis, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        in_feats = resnet.fc.in_features\n",
    "        base_mod = list(resnet.children())[:-1]\n",
    "        self.base_mod = nn.Sequential(*base_mod)\n",
    "        for child in self.base_mod.children():\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.out1 = nn.Linear(in_feats, 512)\n",
    "        self.func1 = nn.LeakyReLU(0.1)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.out2 = nn.Linear(512, 512)\n",
    "        self.func2 = nn.LeakyReLU(0.1)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.out3 = nn.Linear(512, 512)\n",
    "        \n",
    "        self.LSTM = nn.LSTM(input_size = 512, hidden_size = 256, num_layers = 2, batch_first = True)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.func1out = nn.LeakyReLU(0.1)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_embed = []\n",
    "        for i in range(x.size(1)):\n",
    "            x_im = self.base_mod(x[:,i,:,:,:])\n",
    "            x_im = x_im.view(x_im.size(0), -1)\n",
    "            x_im = self.out1(x_im)\n",
    "            x_im = self.func1(x_im)\n",
    "            x_im = self.bn1(x_im)\n",
    "            x_im = self.out2(x_im)\n",
    "            x_im = self.func2(x_im)\n",
    "            x_im = self.bn2(x_im)\n",
    "            x_im = self.out3(x_im)\n",
    "            x_embed.append(x_im)\n",
    "        x_embed = torch.stack(x_embed, dim=0).transpose_(0, 1)\n",
    "        print(x_embed.shape)\n",
    "        out_lstm, (ht, states) = self.LSTM(x_embed)\n",
    "        print('state')\n",
    "        print(ht.shape)\n",
    "        out = out_lstm[:, -1, :]\n",
    "        out = self.fc1(out)\n",
    "        out = self.func1out(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mixed(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers,  glove_weights):\n",
    "        super().__init__()\n",
    "        self.text_emb = BiLSTM(vocab_size, embedding_dim, hidden_dim, num_layers, glove_weights)\n",
    "        self.image = image()\n",
    "        self.video = video_analysis()\n",
    "        \n",
    "        self.categ_1 = nn.Linear(31, 32)\n",
    "        self.funccateg_1 = nn.LeakyReLU(0.1)\n",
    "        self.categ_2 = nn.Linear(32, 32)\n",
    "        \n",
    "        self.numer_1 = nn.Linear(7, 16)\n",
    "        self.funcnumer_1 = nn.LeakyReLU(0.1)\n",
    "        self.numer_2 = nn.Linear(16, 32)\n",
    "        \n",
    "        self.fc1 = nn.Linear(160, 128)\n",
    "        self.func1 = nn.LeakyReLU(0.1)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, 32)\n",
    "        self.func2 = nn.LeakyReLU(0.1)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "    def forward(self, text, l, hour, day, numerical, vid, image):\n",
    "        categ = torch.cat((hour, day),  dim = -1)\n",
    "        categ = self.categ_1(categ)\n",
    "        categ = self.funccateg_1(categ)\n",
    "        categ = self.categ_2(categ)\n",
    "        \n",
    "        num = self.numer_1(numerical)\n",
    "        num = self.funcnumer_1(num)\n",
    "        num = self.numer_2(num)\n",
    "        \n",
    "        text = self.text_emb(text, l)\n",
    "        vid = self.video(vid)\n",
    "        img = self.image(image)\n",
    "        print('text')\n",
    "        print(text.shape)\n",
    "        print('num')\n",
    "        print(num.shape)\n",
    "        print(vid.shape)\n",
    "        print(categ.shape)\n",
    "        print(img.shape)\n",
    "        multi  = torch.cat([num, categ, text, vid, img], axis = -1)\n",
    "        \n",
    "        out = self.fc1(multi)\n",
    "        out = self.func1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.func2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.DataParallel(mixed(vocab_size, 100, 128, 2, W ).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log = []\n",
    "val_log = []\n",
    "import matplotlib.pyplot as plt\n",
    "def train_model(model, epoch, lr):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    opt_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    for i in range(epoch):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        train_log_epoch = []\n",
    "        train_val_epoch = []\n",
    "        i = 0\n",
    "        for elem in train_dl:\n",
    "            i+=1\n",
    "            print(i)\n",
    "            optimizer.zero_grad()\n",
    "            frames = elem[3].cuda()\n",
    "            image = elem[4].cuda()\n",
    "            numbers = elem[0].float().cuda()\n",
    "            hour = elem[1][:,0:24].float().cuda()\n",
    "            day = elem[1][:,24:31].float().cuda()\n",
    "            text = elem[2][0].cuda()\n",
    "            l = elem[2][1]\n",
    "            y = elem[5].float().cuda()\n",
    "            y_pred = model(text, l, hour, day, numbers, frames, image)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss = loss.item()\n",
    "            acc = binary_acc(y_pred, y)\n",
    "            print(acc)\n",
    "            train_log_epoch.append(acc)\n",
    "        train_log.extend(train_log_epoch)\n",
    "        opt_scheduler.step()\n",
    "        a = np.mean(val(model, val_dl))\n",
    "        val_log.append((len(train_log), a))\n",
    "        plot_history(train_log, val_log, title='loss')\n",
    "    return model\n",
    "\n",
    "\n",
    "def val(model, valid_dl):\n",
    "    model.eval()\n",
    "    sum_loss = 0.0\n",
    "    val_epoch = []\n",
    "    for elem in valid_dl:\n",
    "        frames = elem[3].cuda()\n",
    "        image = elem[4].cuda()\n",
    "        numbers = elem[0].float().cuda()\n",
    "        hour = elem[1][:,0:24].float().cuda()\n",
    "        day = elem[1][:,24:31].float().cuda()\n",
    "        text = elem[2][0].cuda()\n",
    "        l = elem[2][1]\n",
    "        y = elem[1].float().cuda()\n",
    "        y_pred = model(text, l, hour, day, numbers, frames, image)\n",
    "        acc = binary_acc(y_pred, y)\n",
    "        val_epoch.append(acc) \n",
    "    return val_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(model, 1,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
