{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import torch\n",
    "import zipfile\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "dataset = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "import string\n",
    "tok = spacy.load('en')\n",
    "def tokenize (text):\n",
    "    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n",
    "    text = no_specials_string = re.sub('[!#?,.:\";]', ' ', text)\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '\\\\r\\\\t\\\\n]')\n",
    "    nopunct = regex.sub(\" \", text.lower())\n",
    "    tmp = [token.text for token in tok.tokenizer(nopunct)]\n",
    "    return [x for x in tmp if not x.isspace()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    dataset['video_title'][i] = tokenize(dataset['video_title'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2id = {\"\":0, \"UNK\":1}\n",
    "words = [\"\", \"UNK\"]\n",
    "\n",
    "from collections import Counter\n",
    "counts_words = Counter()\n",
    "for row in dataset['video_title']:\n",
    "    counts_words.update(row)\n",
    "for word in counts_words:\n",
    "    vocab2id[word] = len(words)\n",
    "    words.append(word)\n",
    "encoded_texts = []\n",
    "dataset['num_words'] = 0\n",
    "for i in range(len(dataset)):\n",
    "    dataset['num_words'][i] = len(dataset['video_title'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[dataset['num_words'] != 0]\n",
    "dataset = dataset.reset_index(drop = True)\n",
    "max_len_title = max(dataset['num_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in dataset['video_title']:\n",
    "    enc = np.zeros(max_len_title)\n",
    "    smth = np.array([vocab2id.get(word, vocab2id[\"UNK\"]) for word in elem])\n",
    "    enc[:len(smth)] = smth\n",
    "    encoded_texts.append(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_texts = np.array(encoded_texts,  dtype =  int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.Series( (v for v in encoded_texts) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Encoded'] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = {}\n",
    "with open(\"glove/glove.twitter.27B.100d.txt\") as f:\n",
    "    for line in f:\n",
    "        arr = line.split()\n",
    "        word_vectors[arr[0]] = np.array([float(x) for x in arr[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(counts_words) + 2\n",
    "vocab_to_idx = {}\n",
    "vocab = [\"\", \"UNK\"]\n",
    "W = np.zeros((vocab_size, 100), dtype=\"float32\")\n",
    "W[0] = np.zeros(100, dtype='float32')\n",
    "W[1] = np.random.uniform(-0.25, 0.25, 100)\n",
    "vocab_to_idx[\"UNK\"] = 1\n",
    "i = 2\n",
    "for word in counts_words:\n",
    "    if word in word_vectors:\n",
    "        W[i] = word_vectors[word]\n",
    "    else:\n",
    "        W[i] = np.random.uniform(-0.25,0.25, 100)\n",
    "    vocab_to_idx[word] = i\n",
    "    vocab.append(word)\n",
    "    i += 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "class text_dataset(data.Dataset):\n",
    "    def __init__(self, text, y):\n",
    "        \"Initialization\"\n",
    "        self.text = text\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def read_images_folder(self, folder):\n",
    "        frames_vid = os.listdir(os.path.join(self.data_path_frame, folder))\n",
    "        def myFunc(e):\n",
    "            return int(e.split('.')[0])\n",
    "        out = []\n",
    "        frames_vid.sort(reverse=False, key=myFunc)\n",
    "        for pic in frames_vid:\n",
    "            image = Image.open(os.path.join(self.data_path_frame, folder, pic))\n",
    "            if self.transform_frames:\n",
    "                image = self.transform_frames(image)\n",
    "            out.append(image)\n",
    "        out = torch.stack(out, dim = 0)\n",
    "        return out\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        encoded_text = torch.from_numpy(self.text[index][0])\n",
    "        length_text = self.text[index][1]\n",
    "        y = list(self.y)[index]\n",
    "        return (encoded_text, length_text), y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "class BiLSTM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers,  glove_weights):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "        self.embeddings.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(input_size = embedding_dim,\n",
    "                            hidden_size = 128,\n",
    "                            bidirectional = True,\n",
    "                            num_layers = num_layers,\n",
    "                            dropout = 0.4,\n",
    "                            batch_first=True)\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.func1 = nn.LeakyReLU(0.1)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, l):\n",
    "        x = self.embeddings(x)\n",
    "        x_pack = pack_padded_sequence(x, l, batch_first=True, enforce_sorted=False)\n",
    "        out_st, (ht, ct) = self.lstm(x_pack)\n",
    "        ht = torch.cat((ht[-2,:,:], ht[-1,:,:]),  dim = 1)\n",
    "        out = self.fc1(ht)\n",
    "        out = self.func1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sig(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['popular'] = 0.0\n",
    "dataset['popular'][dataset.view_count_new.gt(dataset.view_count_new.quantile(0.8))] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.sort_values(by = 'published_at',   ascending=False)\n",
    "dataset = dataset.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = int(0.8*len(dataset))\n",
    "val_ids = int(0.8*len(dataset)) + int(0.1*len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['popular'] = 0.0\n",
    "dataset['popular'][dataset.view_count_new.gt(dataset.view_count_new.quantile(0.8))] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset[:train_ids]\n",
    "val = dataset[train_ids:val_ids]\n",
    "test = dataset[val_ids:len(dataset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train = train[['Encoded', 'num_words']].values\n",
    "y_train = train['popular'].values\n",
    "\n",
    "text_val = val[['Encoded', 'num_words']].values\n",
    "y_val = val['popular'].values\n",
    "\n",
    "text_test = test[['Encoded', 'num_words']].values\n",
    "y_test = test['popular'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = text_dataset(text_train,  y_train)\n",
    "dataset_val = text_dataset(text_val, y_val)\n",
    "dataset_test = text_dataset(text_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = BiLSTM(vocab_size, 100, 128, 2, W ).cuda()\n",
    "model = nn.DataParallel(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = data.DataLoader(dataset_train, batch_size = 200)\n",
    "val_dl = data.DataLoader(dataset_val, batch_size = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(y_pred)\n",
    "    y_pred_tag = y_pred_tag.squeeze(dim = -1)\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = acc.item()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "train_log = []\n",
    "val_log = []\n",
    "import matplotlib.pyplot as plt\n",
    "def train_model(model, epoch, lr):\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    opt_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.5)\n",
    "    criterion = nn.BCELoss()\n",
    "    writer = SummaryWriter()\n",
    "    for i in range(epoch):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        train_log_epoch = []\n",
    "        train_val_epoch = []\n",
    "        num_batch = 0\n",
    "        total_predicts = []\n",
    "        total_corrects = []\n",
    "        for elem in train_dl:\n",
    "            optimizer.zero_grad()\n",
    "            text = elem[0][0].cuda()\n",
    "            length = elem[0][1]\n",
    "            y = elem[1].float().cuda()\n",
    "            y_pred = model(text, length)\n",
    "            print(y.shape)\n",
    "            loss = criterion(y_pred, y.unsqueeze(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss = loss.item()\n",
    "            acc = binary_acc(y_pred, y)\n",
    "            train_log_epoch.append(acc)\n",
    "            y_pred_sig = y_pred.flatten().tolist()\n",
    "            total_predicts.extend(y_pred_sig)\n",
    "            total_corrects.extend(y.flatten().tolist())\n",
    "        \n",
    "        train_log.extend(train_log_epoch)\n",
    "        acc = np.mean(train_log_epoch)\n",
    "        binary_preds = np.array(total_predicts)>0.5\n",
    "        binary_preds = binary_preds.astype(int)\n",
    "        binary_preds = list(binary_preds)\n",
    "        print('train')\n",
    "        print(len(binary_preds))\n",
    "        print(binary_preds)\n",
    "        print(len(total_corrects))\n",
    "        prec = precision_score(total_corrects, binary_preds)\n",
    "        rec = recall_score(total_corrects, binary_preds)\n",
    "        roc_auc = roc_auc_score(total_corrects, total_predicts)\n",
    "        writer.add_scalar('precision_train', prec, i)\n",
    "        writer.add_scalar('recall_train', rec, i)\n",
    "        writer.add_scalar('roc_auc_train', roc_auc, i)\n",
    "        writer.add_scalar('accuracy_train', np.mean(train_log_epoch), i)\n",
    "        prec_test, rec_test, roc_auc_test, acc_test = val(model, val_dl)\n",
    "        opt_scheduler.step(acc_test)\n",
    "        writer.add_scalar('precision_test', prec_test, i)\n",
    "        writer.add_scalar('recall_test', rec_test, i)\n",
    "        writer.add_scalar('roc_auc_test', roc_auc_test, i)\n",
    "        writer.add_scalar('accuracy_test', acc_test, i)\n",
    "        writer.close()\n",
    "    return model\n",
    "\n",
    "\n",
    "def val(model, valid_dl):\n",
    "    model.eval()\n",
    "    sum_loss = 0.0\n",
    "    val_epoch = []\n",
    "    total_preds = []\n",
    "    total_corrects = []\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    for elem in valid_dl:\n",
    "        text = elem[0][0].cuda()\n",
    "        length = elem[0][1]\n",
    "        y = elem[1].float().cuda()\n",
    "        y_pred = model(text, length)\n",
    "        y_pred_sig = y_pred.flatten().tolist()\n",
    "        total_preds.extend(y_pred_sig)\n",
    "        total_corrects.extend(y.flatten().tolist())\n",
    "        acc = binary_acc(y_pred, y)\n",
    "        val_epoch.append(acc)\n",
    "    \n",
    "    binary_preds = np.array(total_preds)>0.5\n",
    "    binary_preds = binary_preds.astype(int)\n",
    "    binary_preds = list(binary_preds)\n",
    "    print('test')\n",
    "    print(total_corrects)\n",
    "    print(binary_preds)\n",
    "    print(total_preds)\n",
    "    prec = precision_score(total_corrects, binary_preds)\n",
    "    rec = recall_score(total_corrects, binary_preds)\n",
    "    roc_auc = roc_auc_score(total_corrects, total_preds)\n",
    "    acc = np.mean(val_epoch)\n",
    "    return prec, rec, roc_auc, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(model, 3, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
